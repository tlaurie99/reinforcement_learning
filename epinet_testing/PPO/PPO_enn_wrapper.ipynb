{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2099f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "import gymnasium as gym\n",
    "from ray import tune, air\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Box\n",
    "import plotly.graph_objects as go\n",
    "from ENNWrapper import ENNWrapper\n",
    "from ray.train import ScalingConfig\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from torch.distributions.normal import Normal\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ray.rllib.core.models.catalog import Catalog\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.core.models.configs import MLPHeadConfig\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.policy.policy_template import build_policy_class\n",
    "from ray.rllib.models.torch.misc import SlimFC, AppendBiasLayer\n",
    "from ray.rllib.utils.annotations import OverrideToImplementCustomLogic\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.typing import Dict, TensorType, List, ModelConfigDict\n",
    "\n",
    "path = os.getcwd()\n",
    "torch, nn = try_import_torch()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736edb5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global adder\n",
    "adder = 1.000001\n",
    "\n",
    "class CustomTorchModelMOG(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        super(CustomTorchModelMOG, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
    "        \n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        \n",
    "        nn.Module.__init__(self)\n",
    "        #get layers from config\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        hidden_layer0 = model_config['fcnet_hiddens'][0]\n",
    "        hidden_layer1 = model_config['fcnet_hiddens'][1]\n",
    "        enn_layer = 50\n",
    "        #object instance variables\n",
    "        self.std = 1.0\n",
    "        self.mean = 0.0\n",
    "        self.gamma = 0.99\n",
    "        self.step_number = 0\n",
    "        self.z_indices = None\n",
    "        self.step_cut_off = 200\n",
    "        self.adder = 1.000000001\n",
    "        # random.seed(self.seed)\n",
    "        self.elu = torch.nn.ELU() \n",
    "        self.action_space = action_space\n",
    "        self.device = torch.device('cpu')\n",
    "        self.num_actions = action_space.shape[0]\n",
    "        self.initializer = torch.nn.init.xavier_normal_\n",
    "        self.activation_fn = model_config['fcnet_activation']\n",
    "        self.z_dim = model_config['custom_model_config'].get('z_dim', 5)\n",
    "        self.num_gaussians = model_config['custom_model_config'].get('num_gaussians', 3)\n",
    "        self.distribution = Normal(torch.full((self.z_dim,), self.mean), \n",
    "                                   torch.full((self.z_dim,), self.std))\n",
    "        self.critic_network = TorchFC(obs_space, action_space, self.num_gaussians*3, \n",
    "                                      model_config, name + \"_critic\")\n",
    "        self.actor_network = TorchFC(obs_space, action_space, action_space.shape[0]*2, \n",
    "                                     model_config, name + \"_actor\")\n",
    "        self.enn_wrapper = ENNWrapper(base_network = self.critic_network, \n",
    "                                      z_dim = self.z_dim, activation_name = self.activation_fn, \n",
    "                                      enn_layer = enn_layer, hidden_layer = hidden_layer0)\n",
    "        self.to(self.device)\n",
    "        self.enn_wrapper.to(self.device)\n",
    "        \n",
    "    @OverrideToImplementCustomLogic\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs_flat'].float().to(self.device)\n",
    "        batch_size = obs.shape[0]\n",
    "        # actor forward pass\n",
    "        raw_action_logits, _ = self.actor_network(input_dict, state, seq_lens)\n",
    "        # use wrapper for critic output / output gradients are blocked - detached so only to update enn\n",
    "        critic_output, self.enn_out = self.enn_wrapper(obs)\n",
    "        # get MoG components\n",
    "        means = critic_output[:, :self.num_gaussians]\n",
    "        sigmas = torch.nn.functional.elu(critic_output[:, self.num_gaussians:self.num_gaussians*2]) + self.adder\n",
    "        alphas = torch.nn.functional.softmax(critic_output[:, self.num_gaussians*2:], dim=-1)\n",
    "        self._u, self._sigmas, self._alphas = means, sigmas, alphas\n",
    "        self.step_number += 1\n",
    "        \n",
    "        return raw_action_logits, state\n",
    "\n",
    "    @OverrideToImplementCustomLogic\n",
    "    def value_function(self):\n",
    "        # get value from critic (for distributional it is means * weights)\n",
    "        multiply = self._u * self._alphas\n",
    "        self.critic_value = torch.sum(multiply.to(self.device), dim = 1)\n",
    "        # add in the ENN to value of critic\n",
    "        self.final_critic_value = self.critic_value.to(self.device) + self.enn_out\n",
    "        return self.final_critic_value.to(self.device)\n",
    "\n",
    "    def predict_gmm_params(self, observation):\n",
    "        output, _ = self.critic_network({\"obs\": observation}, [], None)\n",
    "        means = output[:, :self.num_gaussians]\n",
    "        sigmas_prev = output[:, self.num_gaussians:self.num_gaussians*2]\n",
    "        sigmas = self.elu(sigmas_prev) + self.adder\n",
    "        alphas = output[:, self.num_gaussians*2:]\n",
    "        \n",
    "        return means, sigmas, alphas\n",
    "    \n",
    "    def compute_log_likelihood(self, td_targets, mu_pred, sigma_pred, alphas_pred):\n",
    "        \n",
    "        td_targets_expanded = td_targets.unsqueeze(1)\n",
    "        \n",
    "        sigma_clamped = torch.clamp(sigma_pred, 1e-9, None)\n",
    "        # alphas_clamped = torch.clamp(alpha_pred, 1e-30, 1e5)\n",
    "        \n",
    "        log_2_pi = torch.log(2*torch.tensor(math.pi))\n",
    "        \n",
    "        mus = td_targets_expanded - mu_pred\n",
    "        \n",
    "        logp = torch.clamp(-torch.log(sigma_clamped) - .5 * log_2_pi - torch.square(mus) / (2*torch.square(sigma_clamped)), -1e9, None)\n",
    "        loga = torch.nn.functional.log_softmax(alphas_pred, dim=-1)\n",
    "\n",
    "        summing_log = -torch.logsumexp(logp + loga, dim=-1)\n",
    "        \n",
    "        return summing_log\n",
    "\n",
    "\n",
    "    @OverrideToImplementCustomLogic\n",
    "    def custom_loss(self, policy_loss, sample_batch):\n",
    "        gamma = 0.99\n",
    "        cur_obs = sample_batch[SampleBatch.CUR_OBS]\n",
    "        next_obs = sample_batch[SampleBatch.NEXT_OBS]\n",
    "        rewards = sample_batch[SampleBatch.REWARDS]\n",
    "        dones = sample_batch[SampleBatch.DONES]\n",
    "\n",
    "        mu_pred, sigma_pred, w_pred = self.predict_gmm_params(cur_obs)\n",
    "        mu_target, sigma_target, w_target = self.predict_gmm_params(next_obs)\n",
    "        w_target = torch.nn.functional.softmax(w_target, dim = -1)\n",
    "\n",
    "        \n",
    "        next_state_value = torch.sum(mu_target * w_target, dim = 1).clone().detach()\n",
    "        td_targets = rewards + gamma * next_state_value * (1 - dones.float())\n",
    "        \n",
    "        enn_loss = self.enn_wrapper.enn_loss(next_obs = next_obs, rewards = rewards, \n",
    "                                             dones = dones, current_critic_value = self.final_critic_value,\n",
    "                                             next_critic_value = next_state_value, gamma = gamma)\n",
    "        \n",
    "        log_likelihood = self.compute_log_likelihood(td_targets, mu_pred, sigma_pred, w_pred)\n",
    "        log_likelihood = torch.clamp(log_likelihood, -10, 80)\n",
    "        nll_loss = torch.mean(log_likelihood)\n",
    "        \n",
    "        total_loss = [loss + (nll_loss + enn_loss) for loss in policy_loss]\n",
    "        \n",
    "        if self.step_number % 1_000 == 0:\n",
    "            print(f\"policy loss: {policy_loss} enn loss: {enn_loss} nll loss: {nll_loss}\")\n",
    "    \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "ModelCatalog.register_custom_model(\"custom_torch_model_mog\", CustomTorchModelMOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935974d",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "config = PPOConfig().training(\n",
    "    gamma = 0.99,\n",
    "    lambda_ = 0.95,\n",
    "    # kl_coeff = 0.5,\n",
    "    num_sgd_iter = 30,\n",
    "    lr_schedule = [[0, 0.0003], [15_000_000, 0.00025], [30_000_000, 0.0002], [50_000_000, 0.0001]],\n",
    "    vf_loss_coeff = 1.0,\n",
    "    vf_clip_param = 15.0,\n",
    "    clip_param = 0.3,\n",
    "    grad_clip_by ='norm', \n",
    "    train_batch_size = 19_200, \n",
    "    sgd_minibatch_size = 4_096,\n",
    "    grad_clip = 1.0,\n",
    "    model = {'custom_model': 'custom_torch_model_mog', 'vf_share_layers': False, \n",
    "           'fcnet_hiddens': [2048,2048],'fcnet_activation': 'LeakyReLU'},\n",
    ").environment(env='HalfCheetah-v4'\n",
    ")\n",
    "#.callbacks(MyCustomCallback\n",
    "#)\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "num_iterations = 1\n",
    "results = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    print(f\"Iteration: {i}, Mean Reward: {result['episode_reward_mean']}\")\n",
    "    results.append(result['episode_reward_mean'])\n",
    "    \n",
    "ray.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
