{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b2e90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.logger import pretty_print\n",
    "from models.PyFlytModel import PyFlytModel\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from models.PyFlytModel_MOG import PyFlytModel_MOG\n",
    "from models.PyFlytModel_ENN import PyFlytModel_ENN\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from policies.ppo_torch_policy import SimpleTorchPolicy\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from models.SimpleTorchModel import SimpleCustomTorchModel\n",
    "from utils.normalize_advantages import NormalizeAdvantagesCallback\n",
    "\n",
    "\n",
    "import PyFlyt.gym_envs\n",
    "from ray.tune.registry import register_env\n",
    "from PyFlyt.gym_envs import FlattenWaypointEnv\n",
    "from PyFlyt.gym_envs.quadx_envs import quadx_hover_env, quadx_waypoints_env\n",
    "from PyFlyt.pz_envs.fixedwing_envs.ma_fixedwing_dogfight_env import MAFixedwingDogfightEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4437ac-856e-4a6c-ac86-216fcd8e5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "torch, nn = try_import_torch()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739124bf-1584-469b-aa90-a9e4d3d6553f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    def reward(self, reward):\n",
    "        if reward >= 99.0 or reward <= -99.0:\n",
    "            return reward / 10\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1da5f8-9290-4d99-a020-ba682dd48a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDogfightEnv(MultiAgentEnv):\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.env = MAFixedwingDogfightEnv(**config)\n",
    "        \n",
    "        self.agent_ids = self.env.possible_agents\n",
    "        self.observation_space = self.env.observation_space(self.env.possible_agents[0])\n",
    "        self.action_space = self.env.action_space(self.env.possible_agents[0])\n",
    "\n",
    "    def reset(self):\n",
    "        observations, infos = self.env.reset()\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(actions)\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "# Register the environment\n",
    "def env_creator(config):\n",
    "    return CustomDogfightEnv(config)\n",
    "register_env(\"MAFixedwingDogfightEnv\", env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19d269-4cf4-4213-8700-47a4c113bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def env_creator(env_config):\n",
    "#     return MAFixedwingDogfightEnv(assisted_flight = True)\n",
    "# register_env(\"MAFixedwingDogfightEnv\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae6392-2ad3-401b-b5f9-1f43cfa849a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    # Check if agent_id is a digit\n",
    "    if agent_id.isdigit():\n",
    "        return 'policy_1' if int(agent_id) % 2 == 0 else 'policy_2'\n",
    "    # Handle agent_ids like 'uav_0', 'uav_1', etc.\n",
    "    return 'policy_1' if int(agent_id.split('_')[1]) % 2 == 0 else 'policy_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1fc21-c079-47fa-9eb1-91e76c070273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def env_creator(config):\n",
    "#     return MAFixedwingDogfightEnv(**config)\n",
    "# register_env('MAFixedwingDogfightEnv', lambda config: PettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc6765e-7a9c-4d45-b4f4-49b7ac486483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_creator = lambda config: MAFixedwingDogfightEnv(**config)\n",
    "\n",
    "# register_env('MAFixedwingDogfightEnv', lambda config: PettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092763e-d6b3-43b7-8fef-05b5b87d9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def env_creator(args):\n",
    "#     env = MAFixedwingDogfightEnv.parallel_env(\n",
    "#         spawn_height = 15.0,\n",
    "#         damage_per_hit = 0.02,\n",
    "#         lethal_distance = 15.0,\n",
    "#         lethal_angle_radians = 0.1,\n",
    "#         assisted_flight = True,\n",
    "#         sparse_rewar = False,\n",
    "#         flight_dome_size = 150.0,\n",
    "#         max_duration_seconds = 60.0,\n",
    "#         agent_hz = 30,\n",
    "#         render_mode= None,\n",
    "#     )\n",
    "#     return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9d881-3d1d-47dd-9d3f-81b9b8bae7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'spawn_height': 5.0,\n",
    "    'damage_per_hit': 0.02,\n",
    "    'lethal_distance': 15.0,\n",
    "    'lethal_angle_radians': 0.1,\n",
    "    'assisted_flight': True,\n",
    "    'sparse_reward': False,\n",
    "    'flight_dome_size': 150.0,\n",
    "    'max_duration_seconds': 60.0,\n",
    "    'agent_hz': 30,\n",
    "    'render_mode': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1748fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env_example = env_creator(env_config)\n",
    "obs_space = env_example.observation_space\n",
    "action_space = env_example.action_space\n",
    "\n",
    "config = PPOConfig().training(\n",
    "    gamma = 0.99,\n",
    "    lambda_ = 0.95,\n",
    "    # kl_coeff = 0.5,\n",
    "    num_sgd_iter = 30,\n",
    "    lr_schedule = [[0, 0.0003], [15_000_000, 0.00025], [30_000_000, 0.0002], [50_000_000, 0.0001]],\n",
    "    vf_loss_coeff = 0.5,\n",
    "    vf_clip_param = 15.0,\n",
    "    clip_param = 0.2,\n",
    "    grad_clip_by ='norm', \n",
    "    train_batch_size = 65_000, \n",
    "    sgd_minibatch_size = 4_096,\n",
    "    grad_clip = 0.5,\n",
    "    model = {'custom_model': 'SimpleCustomTorchModel', \n",
    "           'vf_share_layers': False,\n",
    "           'fcnet_hiddens': [256,256],\n",
    "           'fcnet_activation': 'LeakyReLU',\n",
    "             #this isn't used for some models, but doesn't hurt to keep it\n",
    "           'custom_model_config': {\n",
    "                'num_gaussians': 2,\n",
    "               'num_outputs': action_space.shape[0]\n",
    "           }\n",
    "            }\n",
    ").environment(\n",
    "    env = 'MAFixedwingDogfightEnv',\n",
    "    env_config = env_config\n",
    ").rollouts(\n",
    "num_rollout_workers = 28\n",
    ").resources(num_gpus = 1\n",
    ").callbacks(NormalizeAdvantagesCallback\n",
    ").multi_agent(\n",
    "    policies = {\n",
    "        'policy_1': (SimpleTorchPolicy, obs_space, action_space, {}),\n",
    "        'policy_2': (SimpleTorchPolicy, obs_space, action_space, {}),\n",
    "    },\n",
    "    policy_mapping_fn=policy_mapping_fn\n",
    ")\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     'PPO',\n",
    "#     config=config.to_dict(),\n",
    "#     stop={'training_iteration':300},\n",
    "#     checkpoint_freq=10,\n",
    "#     checkpoint_at_end=True,\n",
    "#     # local_dir='./ray_results'\n",
    "# )\n",
    "\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "num_iterations = 300\n",
    "results = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    print(f\"Iteration: {i}, Mean Reward: {result['episode_reward_mean']}\")\n",
    "    results.append([result['episode_reward_mean'], result['episode_len_mean']])\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "    \n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee88e7e-a7d2-45be-ae0b-dbf1a856e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "experiment_type = 'enn_2dim'\n",
    "results_df.to_csv(path + '/logs/test_runs/'+experiment_type+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769858a-cfea-409a-88e9-744b97c51c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a48861-d451-4545-932c-22d8fbf1183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results)\n",
    "plt.title('Training Progress - Mean Reward per Episode')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Reward')\n",
    "# plt.savefig('Basic PPO - HalfCheetah-v4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ff3ab-8fdf-4339-8993-1397615435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd342596-a1c1-43f1-ac46-698d50f8c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlattenWaypointEnv(gym.make(id='PyFlyt/QuadX-Waypoints-v1', flight_mode=-1), context_length=1)\n",
    "\n",
    "obs_list = []\n",
    "obs, info = env.reset()\n",
    "# env.env.env.env.env.drones[0].set_mode(-1)\n",
    "targets = env.unwrapped.waypoints.targets\n",
    "points = np.concatenate((obs[10:13].reshape(-1,3), targets))\n",
    "obs = {'default': obs}\n",
    "obs_list += [obs]\n",
    "\n",
    "reward_list = []\n",
    "action_list = []\n",
    "start = time.time()\n",
    "for i in range(10*40):\n",
    "    compute_action = algo.compute_actions(obs)\n",
    "    action = compute_action['default']\n",
    "    # obs, reward, terminated, truncated, info = env.step(np.zeros((4))+.79)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    obs = {'default': obs}\n",
    "    \n",
    "    obs_list += [obs]\n",
    "    \n",
    "    reward_list += [reward]\n",
    "    action_list += [action]\n",
    "    \n",
    "    if terminated or info['num_targets_reached'] == 4:\n",
    "        break\n",
    "\n",
    "arrays = [d['default'] for d in obs_list]\n",
    "obs_array = np.vstack(arrays)\n",
    "reward_array = np.array(reward_list)\n",
    "action_array = np.array(action_list) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0985d-422f-4b75-b1bf-78ea6500816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_figure = px.scatter_3d(x=obs_array[:,10], y=obs_array[:,11], z=obs_array[:,12], opacity=.6, color=np.arange(len(obs_array)))\n",
    "plotly_figure.add_scatter3d(x=targets[:,0], y=targets[:,1], z=targets[:,2], marker={'color':'green', 'symbol':'square-open', 'size':25, 'line':{'width':10}}, mode='markers')\n",
    "plotly_figure.write_html(path+'/3D_renders/3d_drone_space4_'+experiment_type+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacc4f6-f7af-46c4-b969-4b7fdcf63592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b627b7-6821-479a-a1d5-93c43bd642ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "data_list = []\n",
    "labels = []\n",
    "output_desired = 'length' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if output_desired == 'reward':\n",
    "        data_list.append(df.iloc[:,0])\n",
    "        labels.append(f\"reward for {key}\")\n",
    "    else:\n",
    "        data_list.append(df.iloc[:,1])\n",
    "        labels.append(f\"length for {key}\")\n",
    "\n",
    "for data in data_list:\n",
    "    sns.kdeplot(data, fill = True)\n",
    "\n",
    "plt.legend(title = 'Modes', labels = labels)\n",
    "plt.title(f\"{output_desired}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459393e8-ca4a-4e1c-8980-297a31c6d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "reward = []\n",
    "labels = []\n",
    "output_desired = 'reward' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    plt.scatter(df.iloc[:,0], df.iloc[:,1])\n",
    "    labels.append(f\"length for {key}\")\n",
    "\n",
    "plt.legend(title = 'Different runs', labels = labels)\n",
    "plt.title(f\"{output_desired} over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24efe3-439e-4754-a327-6328958d5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
