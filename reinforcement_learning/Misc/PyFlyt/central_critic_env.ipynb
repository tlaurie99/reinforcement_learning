{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b2e90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from copy import deepcopy\n",
    "import plotly.express as px\n",
    "from gymnasium import spaces\n",
    "from pettingzoo import AECEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from typing import Any, Dict, List\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.logger import pretty_print\n",
    "from models.MOGTorchModel import MOGTorchModel\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from policies.ppo_sb3_loss import CustomLossPolicy\n",
    "# from models.PyFlytModel_MOG import PyFlytModel_MOG\n",
    "# from models.PyFlytModel_ENN import PyFlytModel_ENN\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from policies.ppo_torch_policy import SimpleTorchPolicy\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from models.CentralCriticModel import CentralStackedModel\n",
    "from models.SimpleTorchModel import SimpleCustomTorchModel\n",
    "from models.SimpleTorchModel_param import SimpleCustomTorchModelParam\n",
    "from policies.basic_centralized_critic import CentralCriticPolicy\n",
    "from add_ons.normalize_advantages import NormalizeAdvantagesCallback\n",
    "from ray.rllib.algorithms.ppo.ppo_torch_policy import PPOTorchPolicy\n",
    "from policies.centralized_critic_stack_state import StackedCentralPolicy\n",
    "\n",
    "import PyFlyt.gym_envs\n",
    "from ray.tune.registry import register_env\n",
    "from PyFlyt.gym_envs import FlattenWaypointEnv\n",
    "from PyFlyt.gym_envs.quadx_envs import quadx_hover_env, quadx_waypoints_env\n",
    "from PyFlyt.pz_envs.fixedwing_envs.ma_fixedwing_dogfight_env import MAFixedwingDogfightEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4437ac-856e-4a6c-ac86-216fcd8e5ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "torch, nn = try_import_torch()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1da5f8-9290-4d99-a020-ba682dd48a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDogfightEnv(MultiAgentEnv):\n",
    "    def __init__(self, \n",
    "                 config, \n",
    "                 env: AECEnv = None):\n",
    "\n",
    "        super().__init__()\n",
    "        if env is None:\n",
    "            self.env = MAFixedwingDogfightEnv()\n",
    "        else:\n",
    "            self.env = env\n",
    "        self.env.reset()\n",
    "        self.agent_ids = self.env.possible_agents\n",
    "        self.observation_space = self.env.observation_space(self.env.agents[0])\n",
    "        self.action_space = self.env.action_space(self.env.agents[0])\n",
    "\n",
    "        # self.custom_reward_wrapper = CustomRewardWrapper(self.env)\n",
    "\n",
    "        assert all(\n",
    "            self.env.observation_space(agent) == self.observation_space\n",
    "            for agent in self.env.agents\n",
    "        ), (\n",
    "            \"Observation spaces for all agents must be identical. Perhaps \"\n",
    "            \"SuperSuit's pad_observations wrapper can help (useage: \"\n",
    "            \"`supersuit.aec_wrappers.pad_observations(env)`\"\n",
    "        )\n",
    "\n",
    "        assert all(\n",
    "            self.env.action_space(agent) == self.action_space\n",
    "            for agent in self.env.agents\n",
    "        ), (\n",
    "            \"Action spaces for all agents must be identical. Perhaps \"\n",
    "            \"SuperSuit's pad_action_space wrapper can help (usage: \"\n",
    "            \"`supersuit.aec_wrappers.pad_action_space(env)`\"\n",
    "        )\n",
    "        self._agent_ids = set(self.env.agents)\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset()\n",
    "\n",
    "        obs_dict: MultiAgentDict = {}\n",
    "        infos_1 = {}\n",
    "        infos_2 = {}\n",
    "        for agent_id in observations.keys():\n",
    "            obs_dict[agent_id] = observations[agent_id]\n",
    "            if agent_id == 'uav_0':\n",
    "                infos_1[agent_id] = infos[agent_id]\n",
    "            elif agent_id == 'uav_1':\n",
    "                infos_2[agent_id] = infos[agent_id]\n",
    "        # populate infos dict if it has no data in it\n",
    "        if not infos:\n",
    "            infos = {}\n",
    "        \n",
    "        if 'uav_0' not in infos or not infos['uav_0']:\n",
    "            infos['uav_0'] = {\n",
    "                'wins': np.array([False, False]),\n",
    "                'healths': np.array([1., 1.])\n",
    "            }\n",
    "        \n",
    "        if 'uav_1' not in infos or not infos['uav_1']:\n",
    "            infos['uav_1'] = {\n",
    "                'wins': np.array([False, False]),\n",
    "                'healths': np.array([1., 1.])\n",
    "            }\n",
    "    \n",
    "    \n",
    "        return obs_dict, infos\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(action_dict)\n",
    "\n",
    "        terminations[\"__all__\"] = any(terminations.values())\n",
    "        truncations[\"__all__\"] = any(truncations.values())\n",
    "\n",
    "        obs_dict: MultiAgentDict = {}\n",
    "        reward_dict: MultiAgentDict = {}\n",
    "        termination_dict: MultiAgentDict = {}\n",
    "        \n",
    "        truncation_dict: MultiAgentDict = {}\n",
    "        info_dict: MultiAgentDict = {}\n",
    "\n",
    "        for agent_id in observations.keys():\n",
    "            obs_dict[agent_id] = observations[agent_id]\n",
    "            reward_dict[agent_id] = rewards[agent_id]\n",
    "            termination_dict[agent_id] = terminations[agent_id]\n",
    "            termination_dict['__all__'] = any(termination_dict.values())\n",
    "            truncation_dict[agent_id] = truncations[agent_id]\n",
    "            truncation_dict['__all__'] = any(truncation_dict.values())\n",
    "            info_dict[agent_id] = infos[agent_id]\n",
    "            #populate these info_dicts if no data\n",
    "        if not info_dict or 'uav_0' not in info_dict or 'uav_1' not in info_dict:\n",
    "            info_dict = {\n",
    "                'uav_0': {\n",
    "                    'wins': array([False, False]),\n",
    "                    'healths': array([1., 1.])\n",
    "                },\n",
    "                'uav_1': {\n",
    "                    'wins': array([False, False]),\n",
    "                    'healths': array([1., 1.])\n",
    "                }\n",
    "            }\n",
    "        \n",
    "\n",
    "        # processed_rewards = {\n",
    "        #     agent_id: self.custom_reward_wrapper.reward(reward)\n",
    "        #     for agent_id, reward in rewards.items()\n",
    "        # }\n",
    "            \n",
    "\n",
    "        return obs_dict, reward_dict, termination_dict, truncation_dict, info_dict\n",
    "\n",
    "\n",
    "def env_creator(config):\n",
    "    return CustomDogfightEnv(config)\n",
    "register_env('MAFixedwingDogfightEnv', env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae6392-2ad3-401b-b5f9-1f43cfa849a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    # Check if agent_id is a digit\n",
    "    if agent_id.isdigit():\n",
    "        return 'policy_1' if int(agent_id) % 2 == 0 else 'policy_2'\n",
    "    # Handle agent_ids like 'uav_0', 'uav_1', etc.\n",
    "    return 'policy_1' if int(agent_id.split('_')[1]) % 2 == 0 else 'policy_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9d881-3d1d-47dd-9d3f-81b9b8bae7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'spawn_height': 5.0,\n",
    "    'damage_per_hit': 0.02,\n",
    "    'lethal_distance': 15.0,\n",
    "    'lethal_angle_radians': 0.1,\n",
    "    'assisted_flight': True,\n",
    "    'sparse_reward': False,\n",
    "    'flight_dome_size': 150.0,\n",
    "    'max_duration_seconds': 60.0,\n",
    "    'agent_hz': 30,\n",
    "    'render_mode': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209dfdc6-4b0a-400d-9a1a-6beb9ef6b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# env_example = env_creator(env_config)\n",
    "# obs_space = env_example.observation_space\n",
    "# action_space = env_example.action_space\n",
    "\n",
    "# config = PPOConfig().training(\n",
    "#     gamma = 0.99,\n",
    "#     lambda_ = 0.95,\n",
    "#     # kl_coeff = 0.5,\n",
    "#     num_sgd_iter = 30,\n",
    "#     lr_schedule = [[0, 0.0003], [5_000_000, 0.00020], [10_000_000, 0.00015], [15_000_000, 0.0001]],\n",
    "#     # lr = 0.0003,\n",
    "#     vf_loss_coeff = 0.5,\n",
    "#     # vf_clip_param = 1.0,\n",
    "#     clip_param = 0.3,\n",
    "#     grad_clip_by ='norm', \n",
    "#     train_batch_size = 2_000, \n",
    "#     sgd_minibatch_size = 500,\n",
    "#     grad_clip = 0.5,\n",
    "#     # kl_coeff = 0.01,\n",
    "#     # entropy_coeff = 0.001,\n",
    "#     optimizer = {\n",
    "#         'weight_decay': 0.001\n",
    "#     },\n",
    "#     model = {'custom_model': 'SimpleCustomTorchModel', #SimpleCustomTorchModel MOGTorchModel\n",
    "#            'vf_share_layers': False,\n",
    "#            'fcnet_hiddens': [256,256],\n",
    "#            'fcnet_activation': 'LeakyReLU',\n",
    "#            'custom_model_config': {\n",
    "#                 'num_gaussians': 3,\n",
    "#                 'num_layers': 2,\n",
    "#                 # 'num_outputs': action_space.shape[0],\n",
    "#                 # 'parquet_file_name': 'logs/critic_logging_sigma.parquet',\n",
    "#            }\n",
    "#             }\n",
    "# ).environment(\n",
    "#     env = 'MAFixedwingDogfightEnv',\n",
    "#     env_config = env_config\n",
    "# ).rollouts(\n",
    "# num_rollout_workers = 10\n",
    "# ).resources(num_gpus = 1\n",
    "# ).multi_agent(\n",
    "#     policies = {\n",
    "#         'policy_1': (CustomLossPolicy, obs_space, action_space, {}),\n",
    "#         'policy_2': (CustomLossPolicy, obs_space, action_space, {}),\n",
    "#     },\n",
    "#     policy_mapping_fn=policy_mapping_fn\n",
    "# )\n",
    "\n",
    "# # .callbacks(NormalizeAdvantagesCallback\n",
    "# # )\n",
    "\n",
    "# # analysis = tune.run(\n",
    "# #     'PPO',\n",
    "# #     config=config.to_dict(),\n",
    "# #     stop={'training_iteration':300},\n",
    "# #     checkpoint_freq=10,\n",
    "# #     checkpoint_at_end=True,\n",
    "# #     # local_dir='./ray_results'\n",
    "# # )\n",
    "\n",
    "\n",
    "# algo = config.build()\n",
    "\n",
    "# num_iterations = 1500\n",
    "# results = []\n",
    "\n",
    "# for i in range(num_iterations):\n",
    "#     result = algo.train()\n",
    "#     if i % 10 == 0:\n",
    "#         # print(f\"Iteration: {i}, Mean Reward: {result['env_runners']['episode_reward_mean']} episode length: {result['env_runners']['episode_len_mean']}\")\n",
    "#         print(f\"Iteration: {i}, Policy 1 Mean Reward: {result['env_runners']['policy_reward_mean']['policy_1']} loss: {result['info']['learner']['policy_1']['learner_stats']['total_loss']}\\n\"\n",
    "#               f\"Iteration: {i}, Policy 2 Mean Reward: {result['env_runners']['policy_reward_mean']['policy_2']} loss: {result['info']['learner']['policy_2']['learner_stats']['total_loss']}\\n\"\n",
    "#               f\"Iteration: {i}, episode length: {result['env_runners']['episode_len_mean']}\\n\"\n",
    "#         )\n",
    "\n",
    "#     results.append([result['env_runners']['episode_reward_mean'], result['env_runners']['episode_len_mean']])\n",
    "\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1748fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# change model and policy configurations\n",
    "\n",
    "env_example = env_creator(env_config)\n",
    "obs_space = env_example.observation_space\n",
    "action_space = env_example.action_space\n",
    "\n",
    "config = PPOConfig().training(\n",
    "    gamma = 0.99,\n",
    "    lambda_ = 0.95,\n",
    "    # kl_coeff = 0.5,\n",
    "    num_sgd_iter = 30,\n",
    "    lr_schedule = [[0, 0.0003], [5_000_000, 0.00020], [10_000_000, 0.00015], [15_000_000, 0.0001]],\n",
    "    # lr = 0.0003,\n",
    "    vf_loss_coeff = 0.5,\n",
    "    # vf_clip_param = 1.0,\n",
    "    clip_param = 0.3,\n",
    "    grad_clip_by ='norm', \n",
    "    train_batch_size = 1_500, \n",
    "    sgd_minibatch_size = 250,\n",
    "    grad_clip = 0.5,\n",
    "    # kl_coeff = 0.01,\n",
    "    entropy_coeff = 0.01,\n",
    "    # optimizer = {\n",
    "    #     'weight_decay': 0.001\n",
    "    # },\n",
    ").environment(\n",
    "    env = 'MAFixedwingDogfightEnv',\n",
    "    env_config = env_config\n",
    ").rollouts(\n",
    "num_rollout_workers = 10\n",
    ").resources(num_gpus = 1\n",
    ").multi_agent(\n",
    "    policies={\n",
    "        'policy_1': (StackedCentralPolicy, obs_space, action_space, {\n",
    "            'model': {\n",
    "                'custom_model': 'CentralStackedModel',\n",
    "                'vf_share_layers': False,\n",
    "                'fcnet_hiddens': [256, 256],\n",
    "                'fcnet_activation': 'LeakyReLU',\n",
    "                'custom_model_config': {\n",
    "                    'num_gaussians': 2,\n",
    "                    'num_layers': 2,\n",
    "                    'num_agents': 2,\n",
    "                    'num_frames': 5,\n",
    "                    'vf_clipped_loss': 0.5,\n",
    "                    'opp_action_in_cc': False,\n",
    "                    'global_state_flag': False,\n",
    "                    'gamma': 0.99,                  \n",
    "                }\n",
    "            }\n",
    "        }),\n",
    "        'policy_2': (StackedCentralPolicy, obs_space, action_space, {\n",
    "            'model': {\n",
    "                'custom_model': 'CentralStackedModel',\n",
    "                'vf_share_layers': False,\n",
    "                'fcnet_hiddens': [256, 256],\n",
    "                'fcnet_activation': 'LeakyReLU',\n",
    "                'custom_model_config': {\n",
    "                    'num_gaussians': 2,\n",
    "                    'num_layers': 2,\n",
    "                    'num_agents': 2,\n",
    "                    'num_frames': 5,\n",
    "                    'vf_clipped_loss': 0.5,\n",
    "                    'opp_action_in_cc': False,\n",
    "                    'global_state_flag': False,\n",
    "                    'gamma': 0.99,\n",
    "                    \n",
    "                }\n",
    "            }\n",
    "        }),\n",
    "    },\n",
    "    policy_mapping_fn=policy_mapping_fn\n",
    ")\n",
    "\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "num_iterations = 1500\n",
    "results = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    if i % 10 == 0:\n",
    "        # print(f\"Iteration: {i}, Mean Reward: {result['env_runners']['episode_reward_mean']} episode length: {result['env_runners']['episode_len_mean']}\")\n",
    "        print(f\"Iteration: {i}, Policy 1 Mean Reward: {result['env_runners']['policy_reward_mean']['policy_1']} loss: {result['info']['learner']['policy_1']['learner_stats']['total_loss']}\\n\"\n",
    "              f\"Iteration: {i}, Policy 2 Mean Reward: {result['env_runners']['policy_reward_mean']['policy_2']} loss: {result['info']['learner']['policy_2']['learner_stats']['total_loss']}\\n\"\n",
    "              f\"Iteration: {i}, episode length: {result['env_runners']['episode_len_mean']}\\n\")\n",
    "\n",
    "    results.append([result['env_runners']['episode_reward_mean'], result['env_runners']['episode_len_mean']])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a258a-3fdc-4d35-a4a8-d8938ee7cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(f\"/workspace/pyflyt/logs/log_std_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4313125-6348-488d-b4d0-931b48af21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2368ae1-1740-450e-a26f-a85c9672c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fa4c9-7788-4a59-b615-d33790c5cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = data1[data1<-1]\n",
    "filtered2 = data1[(data1 < 0.0001) & (data1 > -0.0001)]\n",
    "filtered3 = data1[data1>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46671e88-8cc7-4185-85a8-bd58116fd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.dropna(axis = 0, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc228a-2d10-4f83-9756-5e885f2e43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered2.dropna(axis = 0, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9a54b-3f41-453b-98fb-6d2ba34b824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered3.dropna(axis = 0, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f67d7f-5dc8-4968-bea7-38f0de764df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(filtered3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c514d6-123a-4434-a57c-3b01792cd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_0 = filtered3['0'].dropna(axis = 0, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afd4db-cbeb-40b1-a210-b7efafcf1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(action_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7932b18a-f33b-4af7-abee-efd8ccbc136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_logp = pd.read_csv(f\"/workspace/pyflyt/logs/logp_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5ecc6-60af-4320-9b71-73c87caa4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_logp = data_logp[data_logp<-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0cac5-e3b1-433f-b361-ecbf3774049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_logp.dropna(axis = 0, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f91714-9c04-4885-9fbd-6913032f40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(filtered_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b982d-fe16-41d5-8ff2-d005af237e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a9f5f-b388-41a0-962e-5276bf168ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255cea2-5a06-4bd5-8861-78c40a8e9b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743ee32-5e78-4486-9cf1-9af72730f83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b293e-1bab-4822-aaf7-ef4f1d022656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee88e7e-a7d2-45be-ae0b-dbf1a856e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "experiment_type = 'reg_critic_models'\n",
    "results_df.to_csv(path + '/logs/'+experiment_type+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a48861-d451-4545-932c-22d8fbf1183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results)\n",
    "plt.title('Training Progress - Mean Reward per Episode')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Reward')\n",
    "# plt.savefig('Basic PPO - HalfCheetah-v4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fd449-5f7e-4f62-8998-112695a2becc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd342596-a1c1-43f1-ac46-698d50f8c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = FlattenWaypointEnv(gym.make(id='PyFlyt/QuadX-Waypoints-v1', flight_mode=-1), context_length=1)\n",
    "\n",
    "env = CustomDogfightEnv(config=env_config) \n",
    "\n",
    "obs_list = []\n",
    "obs, info = env.reset()\n",
    "# env.env.env.env.env.drones[0].set_mode(-1)\n",
    "obs = obs\n",
    "obs_list += [obs]\n",
    "\n",
    "reward_list = []\n",
    "action_list = []\n",
    "start = time.time()\n",
    "for i in range(10*40):\n",
    "    # compute_action = algo.get_policy('policy_1').compute_actions(obs)\n",
    "    # action = compute_action['default']\n",
    "    # # obs, reward, terminated, truncated, info = env.step(np.zeros((4))+.79)\n",
    "    # obs, reward, terminated, truncated, info = env.step(action)\n",
    "    action_dict = {}\n",
    "    for agent_id in obs:\n",
    "        policy_id = 'policy_1' if agent_id == 'uav_0' else 'policy_2'\n",
    "        input_dict = {\n",
    "            \"obs\": torch.tensor([obs[agent_id]], dtype=torch.float32)  # Convert to tensor\n",
    "        }\n",
    "        compute_action = algo.get_policy(policy_id).compute_actions_from_input_dict(input_dict)\n",
    "        action_dict[agent_id] = compute_action[0][0] \n",
    "        action_dict[agent_id] = np.clip(compute_action[0][0], -1.0, 1.0)  # Clip the action to valid range\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    obs_list += [obs]\n",
    "    \n",
    "    reward_list += [reward]\n",
    "    action_list += [action_dict]\n",
    "    \n",
    "    if terminated[\"__all__\"] or any(info[agent_id].get('collision', False) or info[agent_id].get('out_of_bounds', False) for agent_id in obs):\n",
    "        break\n",
    "\n",
    "arrays = np.array(obs_list)\n",
    "obs_array = np.vstack(arrays)\n",
    "reward_array = np.array(reward_list)\n",
    "action_array = np.array(action_list) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03254c66-daef-46b8-b9f3-ec18fc46f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68081901-1c09-44e0-9e59-6a87fa1892bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_array[20][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d1c10-eaee-4d9e-bf85-0968c8cd5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_array_uav_0 = np.array([obs['uav_0'] for obs in obs_list])\n",
    "obs_array_uav_1 = np.array([obs['uav_1'] for obs in obs_list])\n",
    "\n",
    "# Ensure the extracted array has the correct shape\n",
    "obs_array_uav_0 = np.vstack(obs_array_uav_0)\n",
    "obs_array_uav_1 = np.vstack(obs_array_uav_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0985d-422f-4b75-b1bf-78ea6500816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_uav_0 = go.Scatter3d(\n",
    "    x=obs_array_uav_0[:, 10],\n",
    "    y=obs_array_uav_0[:, 11],\n",
    "    z=obs_array_uav_0[:, 12],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='red',\n",
    "        opacity=0.6\n",
    "    ),\n",
    "    name='uav_0'\n",
    ")\n",
    "scatter_uav_1 = go.Scatter3d(\n",
    "    x=obs_array_uav_1[:, 10],\n",
    "    y=obs_array_uav_1[:, 11],\n",
    "    z=obs_array_uav_1[:, 12],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue',\n",
    "        opacity=0.6\n",
    "    ),\n",
    "    name='uav_1'\n",
    ")\n",
    "fig = go.Figure(data=[scatter_uav_0, scatter_uav_1])\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "fig.write_html('3D_renders/3d_drone_space4_experiment.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacc4f6-f7af-46c4-b969-4b7fdcf63592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b627b7-6821-479a-a1d5-93c43bd642ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "data_list = []\n",
    "labels = []\n",
    "output_desired = 'length' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if output_desired == 'reward':\n",
    "        data_list.append(df.iloc[:,0])\n",
    "        labels.append(f\"reward for {key}\")\n",
    "    else:\n",
    "        data_list.append(df.iloc[:,1])\n",
    "        labels.append(f\"length for {key}\")\n",
    "\n",
    "for data in data_list:\n",
    "    sns.kdeplot(data, fill = True)\n",
    "\n",
    "plt.legend(title = 'Modes', labels = labels)\n",
    "plt.title(f\"{output_desired}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459393e8-ca4a-4e1c-8980-297a31c6d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "reward = []\n",
    "labels = []\n",
    "output_desired = 'reward' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    plt.scatter(df.iloc[:,0], df.iloc[:,1])\n",
    "    labels.append(f\"length for {key}\")\n",
    "\n",
    "plt.legend(title = 'Different runs', labels = labels)\n",
    "plt.title(f\"{output_desired} over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24efe3-439e-4754-a327-6328958d5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
