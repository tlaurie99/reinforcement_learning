{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b2e90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "import seaborn as sns\n",
    "from typing import Any\n",
    "import gymnasium as gym\n",
    "from copy import deepcopy\n",
    "import plotly.express as px\n",
    "from gymnasium import spaces\n",
    "from pettingzoo import AECEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.utils.typing import MultiAgentDict\n",
    "from models.PyFlytModel_MOG import PyFlytModel_MOG\n",
    "from policies.ppo_sb3_loss import CustomLossPolicy\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from policies.ppo_torch_policy import SimpleTorchPolicy\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from models.central_state_model import NLLModelFrameStack\n",
    "from models.SimpleTorchModel import SimpleCustomTorchModel\n",
    "from models.SimpleTorchModel_2 import SimpleCustomTorchModel2\n",
    "from utils.normalize_advantages import NormalizeAdvantagesCallback\n",
    "from ray.rllib.algorithms.ppo.ppo_torch_policy import PPOTorchPolicy\n",
    "from policies.central_state_stacking_policy import CentralStackedPolicy\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "import PyFlyt.gym_envs\n",
    "from ray.tune.registry import register_env\n",
    "from PyFlyt.gym_envs import FlattenWaypointEnv\n",
    "from PyFlyt.gym_envs.quadx_envs import quadx_hover_env, quadx_waypoints_env\n",
    "from PyFlyt.pz_envs.fixedwing_envs.ma_fixedwing_dogfight_env import MAFixedwingDogfightEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4437ac-856e-4a6c-ac86-216fcd8e5ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "torch, nn = try_import_torch()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1da5f8-9290-4d99-a020-ba682dd48a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDogfightEnv(MultiAgentEnv):\n",
    "    def __init__(self, \n",
    "                 config, \n",
    "                 env: AECEnv = None):\n",
    "\n",
    "        super().__init__()\n",
    "        if env is None:\n",
    "            self.env = MAFixedwingDogfightEnv()\n",
    "        else:\n",
    "            self.env = env\n",
    "        self.env.reset()\n",
    "        self.agent_ids = self.env.possible_agents\n",
    "        self.observation_space = self.env.observation_space(self.env.agents[0])\n",
    "        self.action_space = self.env.action_space(self.env.agents[0])\n",
    "\n",
    "        # self.custom_reward_wrapper = CustomRewardWrapper(self.env)\n",
    "\n",
    "        assert all(\n",
    "            self.env.observation_space(agent) == self.observation_space\n",
    "            for agent in self.env.agents\n",
    "        ), (\n",
    "            \"Observation spaces for all agents must be identical. Perhaps \"\n",
    "            \"SuperSuit's pad_observations wrapper can help (useage: \"\n",
    "            \"`supersuit.aec_wrappers.pad_observations(env)`\"\n",
    "        )\n",
    "\n",
    "        assert all(\n",
    "            self.env.action_space(agent) == self.action_space\n",
    "            for agent in self.env.agents\n",
    "        ), (\n",
    "            \"Action spaces for all agents must be identical. Perhaps \"\n",
    "            \"SuperSuit's pad_action_space wrapper can help (usage: \"\n",
    "            \"`supersuit.aec_wrappers.pad_action_space(env)`\"\n",
    "        )\n",
    "        self._agent_ids = set(self.env.agents)\n",
    "\n",
    "        print(\"initialized\")\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset()\n",
    "\n",
    "        obs_dict: MultiAgentDict = {}\n",
    "        infos_1 = {}\n",
    "        infos_2 = {}\n",
    "        for agent_id in observations.keys():\n",
    "            obs_dict[agent_id] = observations[agent_id]\n",
    "            if agent_id == 'uav_0':\n",
    "                infos_1[agent_id] = infos[agent_id]\n",
    "            elif agent_id == 'uav_1':\n",
    "                infos_2[agent_id] = infos[agent_id]\n",
    "        # populate infos dict if it has no data in it\n",
    "            \n",
    "        # if infos[0] is None:\n",
    "        #     infos.append({\n",
    "        #         'uav_0':{\n",
    "        #             'wins': array([False, False]),\n",
    "        #             'healths': array([1., 1.])\n",
    "        #         },\n",
    "        #         'uav_1': {\n",
    "        #             'wins': array([False, False]),\n",
    "        #             'healths': array([1.,1.])\n",
    "        #         }\n",
    "        #     })\n",
    "\n",
    "\n",
    "#infos: {'uav_0': {'wins': array([False, False]), 'healths': array([1., 1.])}, 'uav_1': \n",
    "            #{'wins': array([False, False]), 'healths': array([1., 1.])}}\n",
    "        \n",
    "        return obs_dict, infos_1, infos_2\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(action_dict)\n",
    "\n",
    "        terminations[\"__all__\"] = any(terminations.values())\n",
    "        truncations[\"__all__\"] = any(truncations.values())\n",
    "\n",
    "        obs_dict: MultiAgentDict = {}\n",
    "        reward_dict: MultiAgentDict = {}\n",
    "        termination_dict: MultiAgentDict = {}\n",
    "        \n",
    "        truncation_dict: MultiAgentDict = {}\n",
    "        info_dict: MultiAgentDict = {}\n",
    "\n",
    "        for agent_id in observations.keys():\n",
    "            obs_dict[agent_id] = observations[agent_id]\n",
    "            reward_dict[agent_id] = rewards[agent_id]\n",
    "            termination_dict[agent_id] = terminations[agent_id]\n",
    "            termination_dict['__all__'] = any(termination_dict.values())\n",
    "            truncation_dict[agent_id] = truncations[agent_id]\n",
    "            info_dict[agent_id] = infos[agent_id]\n",
    "            #populate these info_dicts if no data\n",
    "        if info_dict[0] is None:\n",
    "            for agent_id in observations.keys():\n",
    "                infos.append({\n",
    "                    agent_id:{\n",
    "                        'wins': array([False, False]),\n",
    "                        'healths': array([1., 1.])\n",
    "                        }\n",
    "                    })\n",
    "        \n",
    "\n",
    "        # processed_rewards = {\n",
    "        #     agent_id: self.custom_reward_wrapper.reward(reward)\n",
    "        #     for agent_id, reward in rewards.items()\n",
    "        # }\n",
    "        print(f\"obs_dict: {obs_dict}\")\n",
    "        print(f\"reward_dict: {reward_dict}\")\n",
    "        print(f\"termination_dict: {termination_dict}\")\n",
    "        print(f\"truncations: {truncations}\")\n",
    "        print(f\"info_dict: {info_dict}\")\n",
    "        #populate these info_dicts if no data\n",
    "        \n",
    "\n",
    "        return obs_dict, reward_dict, termination_dict, truncations, info_dict\n",
    "\n",
    "\n",
    "def env_creator(config):\n",
    "    return CustomDogfightEnv(config)\n",
    "register_env('MAFixedwingDogfightEnv', env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f816460-b1fc-4c93-9049-d6741813580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def env_creator(env_config):\n",
    "#     import PyFlyt.gym_envs\n",
    "#     from gymnasium import spaces\n",
    "#     env = FlattenWaypointEnv(gym.make('PyFlyt/QuadX-Waypoints-v1', flight_mode=-1), context_length=1)\n",
    "#     env.action_space = spaces.Box(low = np.array(\n",
    "#              [\n",
    "#                  -1.0,\n",
    "#                  -1.0,\n",
    "#                  -1.0,\n",
    "#                  -1.0,\n",
    "#              ]\n",
    "#          ),  high = np.array(\n",
    "#              [\n",
    "#                  1.0,\n",
    "#                  1.0,\n",
    "#                  1.0,\n",
    "#                  1.0,\n",
    "#              ]\n",
    "#          ), dtype=np.float64)\n",
    "#     return env\n",
    "# register_env('PyFlyt/QuadX-Waypoints-v1', env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae6392-2ad3-401b-b5f9-1f43cfa849a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    # Check if agent_id is a digit\n",
    "    if agent_id.isdigit():\n",
    "        return 'policy_1' if int(agent_id) % 2 == 0 else 'policy_2'\n",
    "    # Handle agent_ids like 'uav_0', 'uav_1', etc.\n",
    "    return 'policy_1' if int(agent_id.split('_')[1]) % 2 == 0 else 'policy_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9d881-3d1d-47dd-9d3f-81b9b8bae7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'spawn_height': 5.0,\n",
    "    'damage_per_hit': 0.02,\n",
    "    'lethal_distance': 15.0,\n",
    "    'lethal_angle_radians': 0.1,\n",
    "    'assisted_flight': True,\n",
    "    'sparse_reward': False,\n",
    "    'flight_dome_size': 150.0,\n",
    "    'max_duration_seconds': 60.0,\n",
    "    'agent_hz': 30,\n",
    "    'render_mode': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1748fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# change model and policy configurations\n",
    "\n",
    "env_example = env_creator(env_config)\n",
    "obs_space = env_example.observation_space\n",
    "action_space = env_example.action_space\n",
    "\n",
    "config = PPOConfig().training(\n",
    "    gamma = 0.99,\n",
    "    lambda_ = 0.95,\n",
    "    # kl_coeff = 0.5,\n",
    "    num_sgd_iter = 30,\n",
    "    lr_schedule = [[0, 0.0003], [5_000_000, 0.00020], [10_000_000, 0.00015], [15_000_000, 0.0001]],\n",
    "    # lr = 0.0003,\n",
    "    vf_loss_coeff = 0.5,\n",
    "    # vf_clip_param = 15.0,\n",
    "    clip_param = 0.3,\n",
    "    grad_clip_by ='norm', \n",
    "    train_batch_size = 10_000,\n",
    "    sgd_minibatch_size = 2_500,\n",
    "    grad_clip = 0.5,\n",
    "    # kl_coeff = 0.01,\n",
    "    # entropy_coeff = 0.001,\n",
    "    # optimizer = {\n",
    "    #     'weight_decay': 0.001\n",
    "    # },\n",
    ").environment(\n",
    "    env = 'MAFixedwingDogfightEnv',\n",
    "    env_config = env_config\n",
    ").rollouts(\n",
    "num_rollout_workers = 28\n",
    ").resources(num_gpus = 1\n",
    ").multi_agent(\n",
    "    policies={\n",
    "        'policy_1': (CentralStackedPolicy, obs_space, action_space, {\n",
    "            'model': {\n",
    "                'custom_model': 'NLLModelFrameStack',\n",
    "                'vf_share_layers': False,\n",
    "                'fcnet_hiddens': [256, 256],\n",
    "                'fcnet_activation': 'LeakyReLU',\n",
    "                'custom_model_config': {\n",
    "                    'num_gaussians': 2,\n",
    "                    'num_layers': 2,\n",
    "                    'num_agents': 2,\n",
    "                    'num_frames': 5,\n",
    "                    'vf_clipped_loss': 0.5,\n",
    "                    'opp_action_in_cc': False,\n",
    "                    'global_state_flag': False,\n",
    "                    'gamma': 0.99,\n",
    "                }\n",
    "            }\n",
    "        }),\n",
    "        'policy_2': (CentralStackedPolicy, obs_space, action_space, {\n",
    "            'model': {\n",
    "                'custom_model': 'NLLModelFrameStack',\n",
    "                'vf_share_layers': False,\n",
    "                'fcnet_hiddens': [256, 256],\n",
    "                'fcnet_activation': 'LeakyReLU',\n",
    "                'custom_model_config': {\n",
    "                    'num_gaussians': 3,\n",
    "                    'num_layers': 2,\n",
    "                    'num_agents': 2,\n",
    "                    'num_frames': 5,\n",
    "                    'vf_clipped_loss': 0.5,\n",
    "                    'opp_action_in_cc': False,\n",
    "                    'global_state_flag': False,\n",
    "                    'gamma': 0.99,\n",
    "                }\n",
    "            }\n",
    "        }),\n",
    "    },\n",
    "    policy_mapping_fn=policy_mapping_fn\n",
    ")\n",
    "\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "num_iterations = 1\n",
    "results = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    if i % 10 == 0:\n",
    "        # print(f\"Iteration: {i}, Mean Reward: {result['env_runners']['episode_reward_mean']} episode length: {result['env_runners']['episode_len_mean']}\")\n",
    "        print(f\"Iteration: {i}, Policy 1 Mean Reward: {result['policy_reward_mean']['policy_1']} loss: {result['info']['learner']['policy_1']['learner_stats']['total_loss']}\\n\"\n",
    "              f\"Iteration: {i}, Policy 2 Mean Reward: {result['policy_reward_mean']['policy_2']} loss: {result['info']['learner']['policy_2']['learner_stats']['total_loss']}\\n\"\n",
    "              f\"Iteration: {i}, episode length: {result['episode_len_mean']}\\n\"\n",
    "        )\n",
    "\n",
    "    results.append([result['episode_reward_mean'], result['episode_len_mean']])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdd353-5b75-47e7-aee9-92319ea884c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509618a0-fb84-4ec7-b457-73aa3727e3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933618f-ca62-4a77-96d6-4242dcf25501",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = data['timestep'], y = data['means_min'], mode = 'lines', name = 'min means'))\n",
    "fig.add_trace(go.Scatter(x = data['timestep'], y = data['means_max'], mode = 'lines', name = 'max means'))\n",
    "fig.add_trace(go.Scatter(x = data['timestep'], y = data['log_stds_min'], mode = 'lines', name = 'min log_stds'))\n",
    "fig.add_trace(go.Scatter(x = data['timestep'], y = data['log_stds_max'], mode = 'lines', name = 'max log_stds'))\n",
    "\n",
    "fig.update_layout(title_text=\"Means and Log_stds\", xaxis_title = 'Timesteps', yaxis_title = 'Value',\n",
    "                 font = dict(\n",
    "                     family = 'Times New Roman',\n",
    "                     size = 16\n",
    "                 ),\n",
    "                 width = 900,\n",
    "                 height = 600,\n",
    "                 showlegend = True,\n",
    "                 )\n",
    "# fig.write_image('/loss_function_comparison.png')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99ac1c-1057-4f72-b581-9277f06c51d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a121501-4a4f-4ab0-8f57-4a7e07942d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06b38a-2316-4976-ba95-8b0bc2c672d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59001b5d-de13-4d2e-9a3a-04b3fcbb7e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd342596-a1c1-43f1-ac46-698d50f8c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = FlattenWaypointEnv(gym.make(id='PyFlyt/QuadX-Waypoints-v1', flight_mode=-1), context_length=1)\n",
    "\n",
    "env = CustomDogfightEnv(config=env_config) \n",
    "\n",
    "obs_list = []\n",
    "obs, info = env.reset()\n",
    "# env.env.env.env.env.drones[0].set_mode(-1)\n",
    "obs = obs\n",
    "obs_list += [obs]\n",
    "\n",
    "reward_list = []\n",
    "action_list = []\n",
    "start = time.time()\n",
    "for i in range(1800):\n",
    "    # compute_action = algo.get_policy('policy_1').compute_actions(obs)\n",
    "    # action = compute_action['default']\n",
    "    # # obs, reward, terminated, truncated, info = env.step(np.zeros((4))+.79)\n",
    "    # obs, reward, terminated, truncated, info = env.step(action)\n",
    "    action_dict = {}\n",
    "    for agent_id in obs:\n",
    "        policy_id = 'policy_1' if agent_id == 'uav_0' else 'policy_2'\n",
    "        input_dict = {\n",
    "            \"obs\": torch.tensor([obs[agent_id]], dtype=torch.float32)  # Convert to tensor\n",
    "        }\n",
    "        compute_action = algo.get_policy(policy_id).compute_actions_from_input_dict(input_dict)\n",
    "        action_dict[agent_id] = compute_action[0][0] \n",
    "        action_dict[agent_id] = np.clip(compute_action[0][0], -1.0, 1.0)  # Clip the action to valid range\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action_dict)\n",
    "    \n",
    "    obs_list += [obs]\n",
    "    reward_list += [reward]\n",
    "    action_list += [action_dict]\n",
    "    \n",
    "    if terminated[\"__all__\"] or any(info[agent_id].get('collision', False) or info[agent_id].get('out_of_bounds', False) for agent_id in obs):\n",
    "        break\n",
    "\n",
    "arrays = np.array(obs_list)\n",
    "obs_array = np.vstack(arrays)\n",
    "reward_array = np.array(reward_list)\n",
    "action_array = np.array(action_list) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690a97f-e78e-46bc-baa5-2008270ff538",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_array_uav_0 = np.array([obs['uav_0'] for obs in obs_list])\n",
    "obs_array_uav_1 = np.array([obs['uav_1'] for obs in obs_list])\n",
    "\n",
    "# Ensure the extracted array has the correct shape\n",
    "obs_array_uav_0 = np.vstack(obs_array_uav_0)\n",
    "obs_array_uav_1 = np.vstack(obs_array_uav_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c89ed-9d2a-4d45-bc6e-ef7ce2d517f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_type = 'simple_models_15M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc58a0b-25aa-4ad4-96e0-29b462f8c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_uav_0 = px.scatter_3d(\n",
    "    x=obs_array_uav_0[:, 9],\n",
    "    y=obs_array_uav_0[:, 10],\n",
    "    z=obs_array_uav_0[:, 11],\n",
    "    color=obs_array_uav_0[:, 18],\n",
    ")\n",
    "scatter_uav_0.add_scatter3d(\n",
    "    x=obs_array_uav_0[:, 28],\n",
    "    y=obs_array_uav_0[:, 29],\n",
    "    z=obs_array_uav_0[:, 30],\n",
    "    # color=obs_array_uav_0[:, 31]\n",
    "\n",
    ")\n",
    "scatter_uav_0.write_html(path+'/3D_renders/3d_drone_space4_'+experiment_type+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc2aa1-c357-47ad-90af-2cc55064053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rotation_forward(orn: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes the rotation matrix and forward vector of an aircraft given its orientation.\n",
    "\n",
    "    Args:\n",
    "        orn (np.ndarray): an [n, 3] array of each drone's orientation\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: an [n, 3, 3] rotation matrix of each aircraft\n",
    "        np.ndarray: an [n, 3] forward vector of each aircraft\n",
    "    \"\"\"\n",
    "    c, s = np.cos(orn), np.sin(orn)\n",
    "    eye = np.stack([np.eye(3)] * orn.shape[0], axis=0)\n",
    "\n",
    "    rx = eye.copy()\n",
    "    rx[:, 1, 1] = c[..., 0]\n",
    "    rx[:, 1, 2] = -s[..., 0]\n",
    "    rx[:, 2, 1] = s[..., 0]\n",
    "    rx[:, 2, 2] = c[..., 0]\n",
    "    ry = eye.copy()\n",
    "    ry[:, 0, 0] = c[..., 1]\n",
    "    ry[:, 0, 2] = s[..., 1]\n",
    "    ry[:, 2, 0] = -s[..., 1]\n",
    "    ry[:, 2, 2] = c[..., 1]\n",
    "    rz = eye.copy()\n",
    "    rz[:, 0, 0] = c[..., 2]\n",
    "    rz[:, 0, 1] = -s[..., 2]\n",
    "    rz[:, 1, 0] = s[..., 2]\n",
    "    rz[:, 1, 1] = c[..., 2]\n",
    "\n",
    "    forward_vector = np.stack(\n",
    "        [c[..., 2] * c[..., 1], s[..., 2] * c[..., 1], -s[..., 1]], axis=-1\n",
    "    )\n",
    "\n",
    "    # order of operations for multiplication matters here\n",
    "    return rz @ ry @ rx, forward_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b627b7-6821-479a-a1d5-93c43bd642ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "data_list = []\n",
    "labels = []\n",
    "output_desired = 'length' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if output_desired == 'reward':\n",
    "        data_list.append(df.iloc[:,0])\n",
    "        labels.append(f\"reward for {key}\")\n",
    "    else:\n",
    "        data_list.append(df.iloc[:,1])\n",
    "        labels.append(f\"length for {key}\")\n",
    "\n",
    "for data in data_list:\n",
    "    sns.kdeplot(data, fill = True)\n",
    "\n",
    "plt.legend(title = 'Modes', labels = labels)\n",
    "plt.title(f\"{output_desired}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459393e8-ca4a-4e1c-8980-297a31c6d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "reward = []\n",
    "labels = []\n",
    "output_desired = 'reward' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    plt.scatter(df.iloc[:,0], df.iloc[:,1])\n",
    "    labels.append(f\"length for {key}\")\n",
    "\n",
    "plt.legend(title = 'Different runs', labels = labels)\n",
    "plt.title(f\"{output_desired} over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24efe3-439e-4754-a327-6328958d5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
