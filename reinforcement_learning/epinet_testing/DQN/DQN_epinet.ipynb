{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6d410-4dab-451c-a904-7f64b15fce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.dqn.dqn_torch_model import DQNTorchModel\n",
    "from ray import tune, air\n",
    "from ray.rllib.core.models.configs import MLPHeadConfig\n",
    "from ray.rllib.core.models.catalog import Catalog\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.annotations import OverrideToImplementCustomLogic\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "import ray.rllib.algorithms.ppo as dqn\n",
    "from ray.rllib.utils.typing import Dict, TensorType, List, ModelConfigDict\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.models.torch.misc import SlimFC, AppendBiasLayer\n",
    "from ray.rllib.policy.torch_policy import TorchPolicy\n",
    "from ray.rllib.policy.policy_template import build_policy_class\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.annotations import override\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "import math\n",
    "from torch.distributions.normal import Normal\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.registry import register_env\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import os \n",
    "import random\n",
    "import shutil\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c731a0-994d-4ffe-a802-402036ac4228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e3901-7ab8-4629-b4f9-497c9c9db06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit(gym.Env):\n",
    "    def __init__(self, config=None):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(low=-1.0, high = 1.0, shape=(2,))\n",
    "        self.cur_context = None\n",
    "\n",
    "    def reset(self, *, seed = None, options = None):\n",
    "        self.cur_context = random.choice([-1.0, 1.0])\n",
    "        return np.array([self.cur_context, -self.cur_context]), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        rewards_for_context = {\n",
    "            -1.0: [-10, 0, 10],\n",
    "            1.0: [10, 0, -10],\n",
    "        }\n",
    "        reward = rewards_for_context[self.cur_context][action]\n",
    "        return (\n",
    "            np.array([-self.cur_context, self.cur_context]),\n",
    "            reward,\n",
    "            True,\n",
    "            False,\n",
    "            {'regret': 10 - reward},\n",
    "        )\n",
    "\n",
    "register_env('SimpleContextualBandit', SimpleContextualBandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d48c6-76c6-4e3d-a171-d36ee080a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit(gym.Env):\n",
    "    def __init__(self, config=None):\n",
    "        self.action_space = Discrete(5)\n",
    "        self.observation_space = Box(low=0.0, high = 1.0)\n",
    "        self.num_actions = 5\n",
    "        self.last_action_reward = None\n",
    "\n",
    "    def reset(self, *, seed = None, options = None):\n",
    "        self.last_action_reward = random.choice([0,1])\n",
    "        return np.array([0]), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"Invalid action selection\"\n",
    "        if action < self.num_actions - 1:\n",
    "            reward = np.random.binomial(1,0.5)\n",
    "        else:\n",
    "            reward = self.last_action_reward\n",
    "\n",
    "        print(f'regret: {1 - reward if action < self.num_actions - 1 else 1 - self.last_action_reward}')\n",
    "\n",
    "        return (\n",
    "            np.array([0]), \n",
    "            reward, \n",
    "            True, \n",
    "            False,\n",
    "            {'regret': 1 - reward if action < self.num_actions - 1 else 1 - self.last_action_reward},\n",
    "        )\n",
    "\n",
    "register_env('BernoulliBandit', BernoulliBandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa909d6-573e-4ac9-b042-754a86bc1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENNDQNModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        super(ENNDQNModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
    "        \n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        \n",
    "        nn.Module.__init__(self)\n",
    "        #gpu nonsense\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        hidden_layer0 = model_config['fcnet_hiddens'][0]\n",
    "        hidden_layer1 = model_config['fcnet_hiddens'][1]\n",
    "        enn_layer = 15\n",
    "\n",
    "        #object instance variables\n",
    "        self.z_dim = 5\n",
    "        self.mean = 0.0\n",
    "        self.std = 1.0\n",
    "        self.step_number = 0\n",
    "        self.z_indices = None\n",
    "        self.gamma = 0.99\n",
    "        self.initializer = torch.nn.init.xavier_normal_\n",
    "        self.activation_fn = model_config['fcnet_activation']\n",
    "        self.num_atoms = 1\n",
    "\n",
    "        self.distribution = Normal(torch.full((self.z_dim,), self.mean), torch.full((self.z_dim,), self.std))\n",
    "\n",
    "        self.action_space_size = 4\n",
    "        self.map_size = 8\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.q_network = TorchFC(obs_space, action_space, self.action_space_size*self.num_atoms, model_config, name + \"_q_testing\")\n",
    "        \n",
    "        self.qnetwork_in = SlimFC(obs_space.shape[0], hidden_layer0, initializer=self.initializer, activation_fn=self.activation_fn)\n",
    "        self.qnetwork_1 = SlimFC(hidden_layer0, hidden_layer1, initializer=self.initializer, activation_fn=self.activation_fn)\n",
    "        self.qnetwork_out = SlimFC(hidden_layer1, self.action_space_size*self.num_atoms, initializer=self.initializer, activation_fn=self.activation_fn)\n",
    "        \n",
    "        self.enn_learnable_in = SlimFC(hidden_layer1 + 1, enn_layer, initializer=self.initializer, activation_fn=self.activation_fn)\n",
    "        self.enn_learnable_1 = SlimFC(enn_layer, enn_layer, initializer=self.initializer, activation_fn=self.activation_fn)\n",
    "        self.enn_learnable_out = SlimFC(enn_layer, self.action_space_size*self.num_atoms, initializer=self.initializer, activation_fn=None)\n",
    "\n",
    "        self.prior_in = SlimFC(hidden_layer1 + 1, enn_layer, initializer=self.initializer, activation_fn=self.activation_fn)\n",
    "        self.prior_1 = SlimFC(enn_layer, enn_layer, initializer=self.initializer, activation_fn=self.activation_fn)\n",
    "        self.prior_out = SlimFC(enn_layer, self.action_space_size*self.num_atoms, initializer=self.initializer, activation_fn=None)\n",
    "        \n",
    "\n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "\n",
    "        obs = input_dict[\"obs_flat\"].float()\n",
    "        action_logits, _ = self.q_network(input_dict, state, seq_lens)\n",
    "\n",
    "        batch_size = obs.shape[0]\n",
    "\n",
    "        base_in = self.qnetwork_in(obs)\n",
    "        base_1 = self.qnetwork_1(base_in).to(self.device)\n",
    "        self.base_1_detached = torch.unsqueeze(base_1, 1).detach()\n",
    "        base_out = self.qnetwork_out(base_1)\n",
    "        self.base_value, self.base_indice = torch.max(base_out, dim=-1, keepdim=True)\n",
    "\n",
    "        self.z_indices = self.distribution.sample((batch_size,)).to(self.device)\n",
    "        self.z_unsqueeze = torch.unsqueeze(self.z_indices, -1)\n",
    "        enn_input = torch.cat((self.z_unsqueeze, self.base_1_detached.expand(-1, self.z_dim, -1)), dim=2)\n",
    "        # batch_size, hidden_layer + 1, z_dimensions\n",
    "\n",
    "        if self.step_number < 200:\n",
    "            #prior value (learnable)\n",
    "            prior_in = self.prior_in(enn_input)\n",
    "            prior_1 = self.prior_1(prior_in)\n",
    "            prior = self.prior_out(prior_1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #prior value (static)\n",
    "                prior_in = self.prior_in(enn_input)\n",
    "                prior_1 = self.prior_1(prior_in)\n",
    "                prior = self.prior_out(prior_1)\n",
    "        \n",
    "        # learnable on features and z concat\n",
    "        learnable_in = self.enn_learnable_in(enn_input)\n",
    "        learnable_1 = self.enn_learnable_1(learnable_in)\n",
    "        learnable = self.enn_learnable_out(learnable_1)\n",
    "        learnable_bmm = torch.bmm(torch.transpose(learnable, 1, 2), self.z_unsqueeze)\n",
    "        learnable_out = learnable_bmm.squeeze(-1)\n",
    "        #the above returns (batch_size, num_actions)\n",
    "        #so we select the 'max action' and return the value / index\n",
    "        self.learnable_value = torch.gather(learnable_out, 1, self.base_indice)\n",
    "\n",
    "        # prior on features and z concat\n",
    "        prior_bmm = torch.bmm(torch.transpose(prior, 1, 2), self.z_unsqueeze)\n",
    "        prior_output = prior_bmm.squeeze(-1)\n",
    "        self.prior_value = torch.gather(prior_output, 1, self.base_indice)\n",
    "\n",
    "        self.action_values = base_out + learnable_out + prior_output\n",
    "\n",
    "        self.step_number += 1\n",
    "            \n",
    "        return self.action_values, state\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        #this should be the argmax values added together\n",
    "        return self.learnable_value + self.prior_value + self.base_value\n",
    "\n",
    "\n",
    "    @override(TorchModelV2)\n",
    "    def custom_loss(self, policy_loss, sample_batch):\n",
    "        cur_obs = sample_batch[SampleBatch.CUR_OBS]\n",
    "        next_obs = sample_batch[SampleBatch.NEXT_OBS]\n",
    "        rewards = sample_batch[SampleBatch.REWARDS]\n",
    "        dones = sample_batch[SampleBatch.DONES]\n",
    "\n",
    "\n",
    "        #target critic value\n",
    "        next_base_in = self.qnetwork_in(next_obs)\n",
    "        next_base_1 = self.qnetwork_1(next_base_in)\n",
    "        next_base_1_detached = torch.unsqueeze(next_base_1, 1).detach()\n",
    "        next_base_out = self.qnetwork_out(next_base_1)\n",
    "        next_base_value, next_base_indice = torch.max(next_base_out, dim=-1, keepdim=True)\n",
    "\n",
    "        #enn target value\n",
    "        next_enn_input = torch.cat((self.z_unsqueeze, next_base_1_detached.expand(-1, self.z_dim, -1)), dim=2)\n",
    "        next_learnable_in = self.enn_learnable_in(next_enn_input)\n",
    "        next_learnable_1 = self.enn_learnable_1(next_learnable_in)\n",
    "        next_learnable = self.enn_learnable_out(next_learnable_1)\n",
    "        next_learnable_bmm = torch.bmm(torch.transpose(next_learnable, 1, 2), self.z_unsqueeze)\n",
    "        next_learnable_out = next_learnable_bmm.squeeze(-1)\n",
    "        next_learnable_value = torch.gather(next_learnable_out, 1, self.base_indice)\n",
    "\n",
    "        #with current setup - one iteration is 480 step numbers\n",
    "        if self.step_number < 200:\n",
    "            #prior target value (learnable)\n",
    "            next_prior_in = self.prior_in(next_enn_input)\n",
    "            next_prior_1 = self.prior_1(next_prior_in)\n",
    "            next_prior = self.prior_out(next_prior_1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #prior target value (static)\n",
    "                next_prior_in = self.prior_in(next_enn_input)\n",
    "                next_prior_1 = self.prior_1(next_prior_in)\n",
    "                next_prior = self.prior_out(next_prior_1)\n",
    "\n",
    "        next_prior_bmm = torch.bmm(torch.transpose(next_prior, 1, 2), self.z_unsqueeze)\n",
    "        next_prior_out = next_prior_bmm.squeeze(-1)\n",
    "        next_prior_value = torch.gather(next_prior_out, 1, self.base_indice)\n",
    "        \n",
    "        #enn total (enn = prior(critic_features, z) + enn_learnable(critic_features, z))\n",
    "        enn_out = self.learnable_value + self.prior_value\n",
    "        next_enn_out = next_learnable_value + next_prior_value\n",
    "        #td loss for enn network minus the base network\n",
    "        enn_target = rewards + self.gamma * next_enn_out.detach() * (1 - dones.float())\n",
    "        enn_loss_square = torch.square(enn_out - enn_target)\n",
    "        enn_loss = torch.mean(enn_loss_square)\n",
    "        #detach target from being udpated / base network TD loss\n",
    "        # base_target = rewards + self.gamma * next_base_value.detach() * (1 - dones.float())\n",
    "        # base_square = torch.square(self.base_value - base_target)\n",
    "        # base_loss = torch.mean(base_square)\n",
    "\n",
    "        # l2_lambda = self.lambda_coeff / max(1, self.total_data_seen)\n",
    "\n",
    "        # l2_reg = torch.tensor(0., device=self.device)\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if 'prior' or 'enn' in name:\n",
    "        #         l2_reg += torch.norm(param)\n",
    "        # l2_loss = l2_lambda * l2_reg\n",
    "\n",
    "        total_loss = [loss + enn_loss for loss in policy_loss]\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "\n",
    "ModelCatalog.register_custom_model(\"ENNDQNModel\", ENNDQNModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859aa148-c56c-4630-8a6a-895bce6a4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.make('FrozenLake-v1', is_slippery=True, map_name='4x4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d1c38-0f76-4bbd-93b0-34ef49584e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "config = DQNConfig().training(\n",
    "    num_atoms = 1,\n",
    "    v_min = -10.0,\n",
    "    v_max = 10.0,\n",
    "    noisy = False,\n",
    "    # sigma0 = 0.5,\n",
    "    #sets initial weights for noisy nets\n",
    "    dueling = False,\n",
    "    double_q = False,\n",
    "    n_step = 1, \n",
    "    hiddens = (),\n",
    "    target_network_update_freq = 10,\n",
    "    num_steps_sampled_before_learning_starts = 20,\n",
    "    replay_buffer_config = {\n",
    "        'capacity': 1000\n",
    "    },\n",
    "    #IMPORTANT: need hiddens = [] and dueling = False for parametric action spaces\n",
    "    before_learn_on_batch = None,\n",
    "    training_intensity = None,\n",
    "    td_error_loss_fn = 'huber',\n",
    "    lr = 0.0003,\n",
    "    #td error loss is ignored if num_atoms > 1\n",
    "    categorical_distribution_temperature = 1.0,\n",
    "    optimizer = {\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    #temperature in the range of [0,1] which affects evaluation\n",
    "    model={\n",
    "        'custom_model': 'ENNDQNModel',\n",
    "        'no_final_linear': False,\n",
    "        'fcnet_hiddens': [64,64],\n",
    "        'fcnet_activation': 'relu',\n",
    "        'vf_share_layers': False\n",
    "    }\n",
    ").environment(\n",
    "    env='FrozenLake-v1',\n",
    ").rollouts(\n",
    "    num_rollout_workers = 28,\n",
    ").resources(\n",
    "    num_gpus = 1\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "num_iterations = 500\n",
    "rewards = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    print(f\"Iteration: {i}, Mean Reward: {result['episode_reward_mean']}\")\n",
    "    rewards.append(result['episode_reward_mean'])\n",
    "    \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c4880-a55c-4022-bf90-9bf565646560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
