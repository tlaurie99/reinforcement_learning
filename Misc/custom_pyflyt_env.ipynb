{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b2e90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "import seaborn as sns\n",
    "from typing import Any\n",
    "import gymnasium as gym\n",
    "from copy import deepcopy\n",
    "import plotly.express as px\n",
    "from gymnasium import spaces\n",
    "from pettingzoo import AECEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "# from models.PyFlytModel_MOG import PyFlytModel_MOG\n",
    "# from models.PyFlytModel_ENN import PyFlytModel_ENN\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from policies.ppo_torch_policy import SimpleTorchPolicy\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from models.SimpleTorchModel import SimpleCustomTorchModel\n",
    "from add_ons.normalize_advantages import NormalizeAdvantagesCallback\n",
    "\n",
    "\n",
    "import PyFlyt.gym_envs\n",
    "from ray.tune.registry import register_env\n",
    "from PyFlyt.gym_envs import FlattenWaypointEnv\n",
    "from PyFlyt.gym_envs.quadx_envs import quadx_hover_env, quadx_waypoints_env\n",
    "from PyFlyt.pz_envs.fixedwing_envs.ma_fixedwing_dogfight_env import MAFixedwingDogfightEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4437ac-856e-4a6c-ac86-216fcd8e5ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "torch, nn = try_import_torch()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1da5f8-9290-4d99-a020-ba682dd48a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDogfightEnv(MultiAgentEnv):\n",
    "    def __init__(self, \n",
    "                 config, \n",
    "                 env: AECEnv = None):\n",
    "\n",
    "        super().__init__()\n",
    "        if env is None:\n",
    "            self.env = MAFixedwingDogfightEnv()\n",
    "        else:\n",
    "            self.env = env\n",
    "        self.env.reset()\n",
    "        \n",
    "        self.agent_ids = self.env.possible_agents\n",
    "        self.observation_space = self.env.observation_space(self.env.agents[0])\n",
    "        self.action_space = self.env.action_space(self.env.agents[0])\n",
    "        self.results = []\n",
    "\n",
    "        assert all(\n",
    "            self.env.observation_space(agent) == self.observation_space\n",
    "            for agent in self.env.agents\n",
    "        ), (\n",
    "            \"Observation spaces for all agents must be identical. Perhaps \"\n",
    "            \"SuperSuit's pad_observations wrapper can help (useage: \"\n",
    "            \"`supersuit.aec_wrappers.pad_observations(env)`\"\n",
    "        )\n",
    "\n",
    "        assert all(\n",
    "            self.env.action_space(agent) == self.action_space\n",
    "            for agent in self.env.agents\n",
    "        ), (\n",
    "            \"Action spaces for all agents must be identical. Perhaps \"\n",
    "            \"SuperSuit's pad_action_space wrapper can help (usage: \"\n",
    "            \"`supersuit.aec_wrappers.pad_action_space(env)`\"\n",
    "        )\n",
    "        self._agent_ids = set(self.env.agents)\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, infos = self.env.reset()\n",
    "        \n",
    "        # Ensure observations match the expected shape\n",
    "        formatted_observations = {\n",
    "            agent_id: self._format_observation(obs)\n",
    "            for agent_id, obs in observations.items()\n",
    "        }\n",
    "        \n",
    "        return formatted_observations, infos\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(action_dict)\n",
    "        \n",
    "        # Ensure observations match the expected shape\n",
    "        formatted_observations = {\n",
    "            agent_id: self._format_observation(obs)\n",
    "            for agent_id, obs in observations.items()\n",
    "        }\n",
    "\n",
    "        # Ensure \"__all__\" keys are present in terminations and truncations dictionaries\n",
    "        terminations[\"__all__\"] = any(terminations.values())\n",
    "        truncations[\"__all__\"] = any(truncations.values())\n",
    "\n",
    "        for agent_id, reward in rewards.items():\n",
    "            # print(f\"Agent {agent_id} reward: {reward}\")\n",
    "            self.results.append([agent_id, reward])\n",
    "\n",
    "        return formatted_observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    # def step(self, action_dict):\n",
    "    #     observations, rewards, terminations, truncations, infos = self.env_step(action_dict)\n",
    "        \n",
    "    #     # Ensure observations match the expected shape\n",
    "    #     formatted_observations = {\n",
    "    #         agent_id: self._format_observation(obs)\n",
    "    #         for agent_id, obs in observations.items()\n",
    "    #     }\n",
    "    \n",
    "    #     # Ensure \"__all__\" keys are present in terminations and truncations dictionaries\n",
    "    #     terminations[\"__all__\"] = any(terminations.values())\n",
    "    #     truncations[\"__all__\"] = any(truncations.values())\n",
    "    \n",
    "    #     for agent_id, reward in rewards.items():\n",
    "    #         # Print or log rewards if needed\n",
    "    #         self.results.append([agent_id, reward])\n",
    "    \n",
    "    #     return formatted_observations, rewards, terminations, truncations, infos\n",
    "    \n",
    "    # def env_step(self, actions: dict[str, np.ndarray]) -> tuple[\n",
    "    #         dict[str, Any], dict[str, float], dict[str, bool], dict[str, bool], dict[str, dict[str, Any]]\n",
    "    #     ]:\n",
    "    #     \"\"\"Custom step method for the environment.\"\"\"\n",
    "    #     self.env.past_actions = deepcopy(self.env.current_actions)\n",
    "    #     self.env.current_actions *= 0.0\n",
    "    #     for k, v in actions.items():\n",
    "    #         self.env.current_actions[self.env.agent_name_mapping[k]] = v\n",
    "    #     self.env.aviary.set_all_setpoints(self.env.current_actions)\n",
    "    \n",
    "    #     observations = dict()\n",
    "    #     terminations = {k: False for k in self.env.agents}\n",
    "    #     truncations = {k: False for k in self.env.agents}\n",
    "    #     rewards = {k: 0.0 for k in self.env.agents}\n",
    "    #     infos = {k: dict() for k in self.env.agents}\n",
    "    \n",
    "    #     for _ in range(self.env.env_step_ratio):\n",
    "    #         self.env.aviary.step()\n",
    "    #         for ag in self.env.agents:\n",
    "    #             ag_id = self.env.agent_name_mapping[ag]\n",
    "    #             term, trunc, rew, info = self.env.compute_term_trunc_reward_info_by_id(ag_id)\n",
    "    #             terminations[ag] |= term\n",
    "    #             truncations[ag] |= trunc\n",
    "    #             rewards[ag] += rew\n",
    "    #             infos[ag] = {**infos[ag], **info}\n",
    "    #             observations[ag] = self.env.compute_observation_by_id(ag_id)\n",
    "    \n",
    "    #     self.env.step_count += 1\n",
    "    #     self.env.agents = [\n",
    "    #         agent for agent in self.env.agents\n",
    "    #         if not (terminations[agent] or truncations[agent])\n",
    "    #     ]\n",
    "    \n",
    "    #     return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "\n",
    "    def _format_observation(self, observation):\n",
    "        # Ensure the observation has the correct shape (34,)\n",
    "        if observation.shape[0] == 30:\n",
    "            # Add placeholders for missing components to match the expected shape\n",
    "            observation = np.concatenate((observation, np.zeros(4)))\n",
    "        return observation\n",
    "        \n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "\n",
    "def env_creator(config):\n",
    "    return CustomDogfightEnv(config)\n",
    "register_env('MAFixedwingDogfightEnv', env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae6392-2ad3-401b-b5f9-1f43cfa849a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    # Check if agent_id is a digit\n",
    "    if agent_id.isdigit():\n",
    "        return 'policy_1' if int(agent_id) % 2 == 0 else 'policy_2'\n",
    "    # Handle agent_ids like 'uav_0', 'uav_1', etc.\n",
    "    return 'policy_1' if int(agent_id.split('_')[1]) % 2 == 0 else 'policy_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9d881-3d1d-47dd-9d3f-81b9b8bae7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'spawn_height': 5.0,\n",
    "    'damage_per_hit': 0.02,\n",
    "    'lethal_distance': 15.0,\n",
    "    'lethal_angle_radians': 0.1,\n",
    "    'assisted_flight': True,\n",
    "    'sparse_reward': False,\n",
    "    'flight_dome_size': 150.0,\n",
    "    'max_duration_seconds': 60.0,\n",
    "    'agent_hz': 30,\n",
    "    'render_mode': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1748fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env_example = env_creator(env_config)\n",
    "obs_space = env_example.observation_space\n",
    "action_space = env_example.action_space\n",
    "\n",
    "config = PPOConfig().training(\n",
    "    gamma = 0.99,\n",
    "    lambda_ = 0.95,\n",
    "    # kl_coeff = 0.5,\n",
    "    num_sgd_iter = 15,\n",
    "    # lr_schedule = [[0, 0.0003], [15_000_000, 0.00025], [30_000_000, 0.0002], [50_000_000, 0.0001]],\n",
    "    lr = 0.0001,\n",
    "    vf_loss_coeff = 0.5,\n",
    "    vf_clip_param = 15.0,\n",
    "    clip_param = 0.2,\n",
    "    grad_clip_by ='norm', \n",
    "    train_batch_size = 12_000, \n",
    "    sgd_minibatch_size = 3_000,\n",
    "    grad_clip = 0.5,\n",
    "    # model = {'custom_model': 'SimpleCustomTorchModel', \n",
    "    #        'vf_share_layers': False,\n",
    "    #        'fcnet_hiddens': [256,256],\n",
    "    #        'fcnet_activation': 'LeakyReLU',\n",
    "    #          #this isn't used for some models, but doesn't hurt to keep it\n",
    "    #        'custom_model_config': {\n",
    "    #             'num_gaussians': 2,\n",
    "    #            'num_outputs': action_space.shape[0]\n",
    "    #        }\n",
    "    #         }\n",
    ").environment(\n",
    "    env = 'MAFixedwingDogfightEnv',\n",
    "    env_config = env_config\n",
    ").rollouts(\n",
    "num_rollout_workers = 10\n",
    ").resources(num_gpus = 1\n",
    ")\n",
    "\n",
    "# .callbacks(NormalizeAdvantagesCallback\n",
    "# )\n",
    "\n",
    "# .multi_agent(\n",
    "#     policies = {\n",
    "#         'policy_1': (SimpleTorchPolicy, obs_space, action_space, {}),\n",
    "#         'policy_2': (SimpleTorchPolicy, obs_space, action_space, {}),\n",
    "#     },\n",
    "#     policy_mapping_fn=policy_mapping_fn\n",
    "# )\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     'PPO',\n",
    "#     config=config.to_dict(),\n",
    "#     stop={'training_iteration':300},\n",
    "#     checkpoint_freq=10,\n",
    "#     checkpoint_at_end=True,\n",
    "#     # local_dir='./ray_results'\n",
    "# )\n",
    "\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "num_iterations = 750\n",
    "results = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration: {i}, Mean Reward: {result['env_runners']['episode_reward_mean']} episode length: {result['env_runners']['episode_len_mean']}\")\n",
    "    results.append([result['env_runners']['episode_reward_mean'], result['env_runners']['episode_len_mean']])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee88e7e-a7d2-45be-ae0b-dbf1a856e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "experiment_type = 'enn_2dim'\n",
    "results_df.to_csv(path + '/logs/test_runs/'+experiment_type+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769858a-cfea-409a-88e9-744b97c51c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a48861-d451-4545-932c-22d8fbf1183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results)\n",
    "plt.title('Training Progress - Mean Reward per Episode')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Reward')\n",
    "# plt.savefig('Basic PPO - HalfCheetah-v4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ff3ab-8fdf-4339-8993-1397615435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd342596-a1c1-43f1-ac46-698d50f8c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlattenWaypointEnv(gym.make(id='PyFlyt/QuadX-Waypoints-v1', flight_mode=-1), context_length=1)\n",
    "\n",
    "obs_list = []\n",
    "obs, info = env.reset()\n",
    "# env.env.env.env.env.drones[0].set_mode(-1)\n",
    "targets = env.unwrapped.waypoints.targets\n",
    "points = np.concatenate((obs[10:13].reshape(-1,3), targets))\n",
    "obs = {'default': obs}\n",
    "obs_list += [obs]\n",
    "\n",
    "reward_list = []\n",
    "action_list = []\n",
    "start = time.time()\n",
    "for i in range(10*40):\n",
    "    compute_action = algo.compute_actions(obs)\n",
    "    action = compute_action['default']\n",
    "    # obs, reward, terminated, truncated, info = env.step(np.zeros((4))+.79)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    obs = {'default': obs}\n",
    "    \n",
    "    obs_list += [obs]\n",
    "    \n",
    "    reward_list += [reward]\n",
    "    action_list += [action]\n",
    "    \n",
    "    if terminated or info['num_targets_reached'] == 4:\n",
    "        break\n",
    "\n",
    "arrays = [d['default'] for d in obs_list]\n",
    "obs_array = np.vstack(arrays)\n",
    "reward_array = np.array(reward_list)\n",
    "action_array = np.array(action_list) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0985d-422f-4b75-b1bf-78ea6500816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_figure = px.scatter_3d(x=obs_array[:,10], y=obs_array[:,11], z=obs_array[:,12], opacity=.6, color=np.arange(len(obs_array)))\n",
    "plotly_figure.add_scatter3d(x=targets[:,0], y=targets[:,1], z=targets[:,2], marker={'color':'green', 'symbol':'square-open', 'size':25, 'line':{'width':10}}, mode='markers')\n",
    "plotly_figure.write_html(path+'/3D_renders/3d_drone_space4_'+experiment_type+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacc4f6-f7af-46c4-b969-4b7fdcf63592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b627b7-6821-479a-a1d5-93c43bd642ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "data_list = []\n",
    "labels = []\n",
    "output_desired = 'length' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if output_desired == 'reward':\n",
    "        data_list.append(df.iloc[:,0])\n",
    "        labels.append(f\"reward for {key}\")\n",
    "    else:\n",
    "        data_list.append(df.iloc[:,1])\n",
    "        labels.append(f\"length for {key}\")\n",
    "\n",
    "for data in data_list:\n",
    "    sns.kdeplot(data, fill = True)\n",
    "\n",
    "plt.legend(title = 'Modes', labels = labels)\n",
    "plt.title(f\"{output_desired}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459393e8-ca4a-4e1c-8980-297a31c6d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for filename in os.listdir(path+'/logs/test_runs'):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path+'/logs/test_runs', filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "reward = []\n",
    "labels = []\n",
    "output_desired = 'reward' #else will give length\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    plt.scatter(df.iloc[:,0], df.iloc[:,1])\n",
    "    labels.append(f\"length for {key}\")\n",
    "\n",
    "plt.legend(title = 'Different runs', labels = labels)\n",
    "plt.title(f\"{output_desired} over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24efe3-439e-4754-a327-6328958d5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
